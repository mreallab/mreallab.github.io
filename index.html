<!DOCTYPE html>
<html class="no-js" lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>MReaL Lab</title>
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
    <link rel="icon" href="favicon.ico" type="image/x-icon">
    <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,900" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/styles.css" rel="stylesheet">
    <style>.bluetext {color: blue;}
</style>
</head>

<body>
    <div id="mobile-menu-open" class="shadow-large">
        <i class="fa fa-bars" aria-hidden="true"></i>
    </div>
    <!-- End #mobile-menu-toggle -->
    <header>
        <div class="header">
            <div class="container">
                <ul class="nav navbar-nav navbar-right">
                    <li><a href="#" class="logo"><img src="images/logo.png" height="30px" alt="" /></a></li>
                    <li><a onclick="window.location='./index.html#experience'"> News </a></li>
                    <li><a onclick="window.location='./people.html'"> People </a></li>
                    <li><a onclick="window.location='./publications.html'"> Publications </a></li>
                    <li><a onclick="window.location='./resources.html'"> Resources </a></li>
                    <li><a onclick="window.location='./index.html#contact'"> Contact </a></li>
                </ul>
            </div>
        </div>
    </header>
    <!-- End header -->

    <div id="about">
        <div class="container">
            <div class="row">
                <div class="col-md-3">
                    <h2 class="heading"><img src="images/logo.png" width="200px" alt="" /></h2>
                </div>
                <div class="col-md-9">
                    <p>
                        <br>
                        Welcome to <b class="bluetext">MReaL</b>! (Machine Reasoning and Learning, pronounced Me Real). Current AI is
                        substantially different from human intelligence in crucial ways because our mind is bicameral:
                        the right brain hemisphere is for perception, which is similar to existing deep learning
                        systems; the left hemisphere is for logic reasoning; and the two of them work so differently and
                        collaboratively that yield creative intelligence. To this end, at <b class="bluetext">MReaL</b>, we are seeking
                        in-principle reasoning algorithms that take the complementary advantages of the modern deep
                        neural networks for learning representations and the old-school symbolic operations for
                        reasoning.
                    </p>
                </div>
            </div>
        </div>
    </div>
    <!-- End #about -->

    <div id="experience" class="background-alt">
        <h2 class="heading"><b>News</b></h2>
        <div id="experience-timeline">
            <div data-date="June 2025">
                <h4><b>5 Papers Accepted by ICCV 2025 </b></h4>
                <li>Dynamic Multimodal Prototype Learning in Vision-Language Models</li>
                <li>Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization</li>
                <li>Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models</li>
                <li>Nautilus: Locality-aware Autoencoder for Scalable Mesh Generation</li>
                <li>Corvid: Improving Multimodal Large Language Models Towards Chain-of-Thought Reasoning</li>
            </div>
            
            <div data-date="May 2025">
                <h4><b>One Paper Accepted by TPAMI </b></h4>
                <li>Gamba: Marry Gaussian Splatting with Mamba for Single-View 3D Reconstruction</li>
            </div>
            
            <div data-date="May 2025">
                <h4><b>4 Papers Accepted by CVPR 2025 (One spotlight) </b></h4>
                <li>On Path to Multimodal Generalist: Levels and Benchmarks [<em class="bluetext">Spotlight</em>]</li>
                <li>3D Question Answering via only 2D Vision-Language Models</li>
                <li>VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models</li>
                <li>Diffusion Model with Causal Generation and Cache Sharing</li>
            </div>
            
            <div data-date="Mar 2025">
                <h4><b>6 Papers Accepted by CVPR 2025 (Two orals, Three highlights) </b></h4>
                <li>Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness [<em class="bluetext">Highlight</em>]</li>
                <li>A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training</li>
                <li>Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene [<em class="bluetext">Highlight</em>] </li>
                <li>Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens [<em class="bluetext"> Oral</em>] </li>
                <li>CARE Transformer: Mobile-Friendly Linear Visual Transformer via Decoupled Dual Interaction [<em class="bluetext">Highlight</em>] </li>
                <li>AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea [<em class="bluetext"> Oral</em>]</li>
            </div>
            
            <div data-date="Jan 2025">
                <h4><b>One Paper Accepted by ICLR 2025 </b></h4>
                <li>Towards Semantic Equivalence of Tokenization in Multimodal LLM</li>
            </div>
            
            <div data-date="Dec 2024">
                <h4><b>One Paper Accepted by AAAI 2025 </b></h4>
                <li>SGDiff: Scene Graph Guided Diffusion Model for Image Collaborative SegCaptioning</li>
            </div>
            
            <div data-date="Sep 2024">
                <h4><b>9 Papers Accepted by NeurIPS 2024 (2 Spotlights) </b></h4>
                <li>Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration[<em
                    class="bluetext">spotlight</em>]</li>
                <li>Enhancing Zero-Shot Vision Models by Label-Free Prompt Distribution Learning and Bias Correcting[<em
                    class="bluetext">spotlight</em>]</li>
                <li>Robust Fine-tuning of Zero-shot Models via Variance Reduction</li>
                <li>Unified Generative and Discriminative Training for Multi-modal Large Language Models</li>
                <li>Vitron: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing</li>
                <li>MVGamba: Unify 3D Content Generation as State Space Sequence Modeling</li>
                <li>Decoupled Kullback-Leibler Divergence Loss</li>
                <li>Lever LM: Configuring In-Context Sequence to Lever Large Vision Language Models</li>
                <li>Action Imitation in Common Action Space for Customized Action Image Synthesis</li>
            </div>
            
             <div data-date="July 2024">
                <h4><b>4 Papers Accepted by ECCV 2024 </b></h4>
                <li>Rethinking and Improving Visual Prompt Selection for In-Context Learning Segmentation Framework</li>
                <li>Instruction Tuning-free Visual Token Complement for Multimodal LLMs</li>
                <li>Few-shot NeRF by Adaptive Rendering Loss Regularization</li>
                <li>View-Consistent 3D Editing with Gaussian Splatting</li>
            </div> 
            <div data-date="May 2024">
                <h4><b>3 Papers Accepted by ICML 2024 (One Oral and One Spotlight) </b></h4>
                <li>Auto-Encoding Morph-Tokens for Multimodal LLM [<em
                    class="bluetext">spotlight</em>]</li>
                <li>Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition [<em
                    class="bluetext">oral</em>]</li>
                <li>Non-confusing Generation of Customized Concepts in Diffusion Models</li>
            </div> 

            <div data-date="May 2024">
                <h4><b>One Paper Accepted by TPAMI </b></h4>
                <li>NICEST: Noisy Label Correction and Training for Robust Scene Graph Generation</li>
            </div> 
            
            <div data-date="Feb 2024">
                <h4><b>9 Papers Accepted by CVPR 2024 </b></h4>
                <li>Diffusion Time-step Curriculum for One Image to 3D Generation</li>
                <li>Distributionally Generative Augmentation for Fair Facial Attribute Classification</li>
                <li>Doubly Abductive Counterfactual Inference for Text-based Image Editing</li>
                <li>Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior</li>
                <li>Few‑shot Learner Parameterization by Diffusion Time‑steps</li>
                <li>Discriminative Probing and Tuning for Text-to-Image Generation</li>
                <li>Empowering Dynamics-aware Text-to-Video Diffusion with LLMs</li>
                <li>Classes Are Not Equal: An Empirical Study on Image Recognition Fairness</li>
                <li>DisCo: Disentangled Control for Realistic Human Dance Generation</li>
            </div>
            
            <div data-date="Jan 2024">
                <h4><b>2 Papers Accepted by ICLR 2024 </b></h4>
                <li> Exploring Diffusion Time-steps for Unsupervised Representation Learning </li>
                <li> Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions </li>
            </div>
            
            <div data-date="Dec 2023">
                <h4><b>2 Papers Accepted by AAAI 2024 </b></h4>
                <li> Dual-Perspective Knowledge Enrichment for Semi-Supervised 3D Object Detection </li>
                <li> MGNet:  Learning Correspondences via Multiple Graphs </li>
            </div>
            
            <div data-date="Sep 2023">
                <h4><b>4 Papers Accepted by NeurIPS 2023 </b></h4>
                <li> Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models </li>
                <li> Tuning Multi-mode Token-level Prompt Alignment across Modalities </li>
                <li> Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation </li>
                <li> Imagine That! Abstract-to-Intricate Text-to-Image Synthesis with Scene Graph Hallucination Diffusion.  </li>
            </div>
            
            <div data-date="July 2023">
                <h4><b>7 Papers Accepted by ICCV 2023 </b></h4>
                <li> Equivariant Similarity for Vision-Language Foundation Models </li>
                <li> Prompt-aligned Gradient for Prompt Tuning </li>
                <li> Random Boxes Are Open-world Object Detectors </li>
                <li> Invariant Feature Regularization for Fair Face Recognition </li>
                <li> Invariant Training 2D-3D Joint Hard Samples for Few-Shot Point Cloud Recognition </li>
                <li> Mitigating and Evaluating Static Bias of Action Representations in the Background and the Foreground </li>
                <li> Learning Trajectory-Word Alignments for Video-Language Tasks </li>
            </div>
            
            <div data-date="Jun 2023">
                <h4><b>One Paper Accepted by TPAMI </b></h4>
                <li> Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering </li>
            </div>

            <div data-date="May 2023">
                <h4><b>Two Papers Accepted by ACL 2023 </b></h4>
                <li> Counterfactual Active Learning for Out-of-Distribution Generalization </li>
                <li> Hypothetical Training for Robust Machine Reading Comprehension of Tabular Context </li>
            </div>
            
            <div data-date="Feb 2023">
                <h4><b>Three Papers Accepted by CVPR 2023</b></h4>
                <li> Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly Detection </li>
                <li> Bootstrap Your Own Prior: Towards Distribution-Agnostic Novel Class Discovery </li>
                <li> Semantic Scene Completion with Cleaner Self </li>
            </div>
            
            <div data-date="Jan 2023">
                <h4><b>One Paper Accepted by ICLR 2023</b></h4>
                <li>Compositional Prompt Tuning with Motion Cues for Open-vocabulary Video Relation Detection </li>
            </div>

            <div data-date="Nov 2022">
                <h4><b>One Paper Accepted by AAAI 2023</b></h4>
                <li>Debiased Fine-Tuning for Vision-language Models by Prompt Regularization </li>
            </div>
            
            <div data-date="Oct 2022">
                <h4><b>One Paper Accepted by NeurIPS 2022</b></h4>
                <li>Respecting Transfer Gap in Knowledge Distillation </li>
            </div>
            
            <div data-date="Oct 2022">
                <h4><b>One Paper Accepted by IJCV</b></h4>
                <li>Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning </li>
            </div>
            
            <div data-date="July 2022">
                <h4><b>Four Papers Accepted by ECCV 2022 (One Oral)</b></h4>
                <li>Identifying Hard Noise in Long-Tailed Sample Distribution[<em
                    class="bluetext">oral</em>]</li>
                <li>Invariant Feature Learning for Generalized Long-Tailed Classification</li>
                <li>Class Is Invariant to Context and Vice Versa: On Learning Invariance for Out-Of-Distribution Generalization</li>
                <li>Equivariance and Invariance Inductive Bias for Learning from Insufficient Data</li>
            </div>
            
            <div data-date="May 2022">
                <h4><b>One Paper Accepted by ICML 2022</b></h4>
                <li>Certified Robustness Against Natural Language Attacks by Causal Intervention</li>
            </div>
            
            <div data-date="March 2022">
                <h4><b>One Paper Accepted by CVPR 2022</b></h4>
                <li>Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation</li>
            </div>
            
            <div data-date="Feb 2022">
                <h4><b>Two Papers Accepted by ACL 2022</b></h4>
                <li>Cross-KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base</li>
                <li>Learning to Imagine: Integrating Counterfactual Thinking in Neural Discrete Reasoning</li>
            </div>
            
            <div data-date="Jan 2022">
                <h4><b>One Paper Accepted by ICLR 2022</b></h4>
                <li>On Non-Random Missing Labels in Semi-Supervised Learning</li>
            </div>
            
            <div data-date="Dec 2021">
                <h4><b>Two Papers (Two Orals) Accepted by AAAI 2022</b></h4>
                <li>Cross-Domain Empirical Risk Minimization for Unbiased Long-tailed Classification[<em
                    class="bluetext">oral</em>]</li>
                <li>Deconfounded Visual Grounding[<em
                    class="bluetext">oral</em>]</li>
            </div>
            
            <div data-date="Oct 2021">
                <h4><b>One Paper Accepted by TPAMI 2022</b></h4>
                <li>Deconfounded image captioning: A causal retrospect</li>
            </div>
            
            <div data-date="Sep 2021">
                <h4><b>Three Papers (One Spotlight) Accepted by NeurIPS 2021</b></h4>
                <li>Self-Supervised Learning Disentangled Group Representation as Feature [<em
                    class="bluetext">spotlight</em>]</li>
                <li>How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?</li>
                <li>Introspective Distillation for Robust Question Answering</li>
            </div>
            
            <div data-date="Aug 2021">
                <h4><b>One Paper Accepted by EMNLP 2021</b></h4>
                <li>TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph</li>
            </div>
            <div data-date="Jan 2021">
                <h4><b>Four Papers (One Oral) Accepted by ICCV 2021</b></h4>
                <li>Transporting Causal Mechanisms for Unsupervised Domain Adaptation [<em
                    class="bluetext">oral</em>]</li>
                <li>Causal Attention for Unbiased Visual Recognition</li>
                <li>Self-Regulation for Semantic Segmentation</li>
                <li>Auto-Parsing Network for Image Captioning and Visual Question Answering</li>
            </div>

            <div data-date="Jan 2021">
                <h4><b>1st Causality in Vision Workshop, CVPR 2021</b></h4>
                <li>Our group will host the 1st Causality in Vision Workshop on CVPR 2021</li>
                <li>Workshop Homepage: <a href="http://www.causalityinvision.com/">http://www.causalityinvision.com/</a></li>
            </div>

            <div data-date="Jan 2021">
                <h4><b>Five Papers Accepted by CVPR 2021</b></h4>
                <li>Distilling Causal Effect of Data in Class-Incremental Learning</li>
                <li>Counterfactual VQA: A Cause-Effect Look at Language Bias</li>
                <li>Counterfactual Zero-Shot and Open-Set Visual Recognition</li>
                <li>Causal Attention for Vision-Language Tasks</li>
                <li>The Blessings of Unlabeled Background in Untrimmed Videos</li>
            </div>

            <div data-date="Jan 2021">
                <h4><b>One Paper Accepted by TMM 2021</b></h4>
                <li>Align R-CNN: A Pairwise Head Network for Visual Relationship Detection</li>
            </div>

            <div data-date="Dec 2020">
                <h4><b>Two Papers Accepted by TPAMI and AAAI 2021</b></h4>
                <li>Auto-encoding and Distilling Scene Graphs for Image Captioning</li>
                <li>Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding</li>
            </div>

            <div data-date="Sep 2020">
                <h4><b>Three Papers (One Oral) Accepted by NeurIPS 2020</b></h4>
                <li>Causal Intervention for Weakly-Supervised Semantic Segmentation [<em
                    class="bluetext">oral</em>]</li>
                <li>Interventional Few-Shot Learning</li>
                <li>Long-Tailed Classification by Keeping the Good and Removing the Bad Momentum Causal Effect</li>
            </div>

            <div data-date="Sep 2020">
                <h4><b>One Paper Accepted by ACM-MM 2020</b></h4>
                <li>Hierarchical Scene Graph Encoder-Decoder for Image Paragraph Captioning</li>
            </div>

            <div data-date="July 2020">
                <h4><b>Two Papers Accepted by ECCV and TMM</b></h4>
                <li>Feature Pyramid Transformer, ECCV</li>
                <li>Self-Adaptive Neural Module Transformer for Visual Question Answering, TMM</li>
            </div>

            <div data-date="Mar 2020">
                <h4><b>Eight Papers (Two Oral) Accepted by CVPR 2020</b></h4>
                <li>Iterative Context-Aware Graph Inference for Visual Dialog [<em
                    class="bluetext">oral</em>]</li>
                <li>Unbiased Scene Graph Generation from Biased Training [<em
                    class="bluetext">oral</em>]</li>
                <li>Visual Commonsense R-CNN</li>
                <li>Learning to Segment the Tail</li>
                <li>Two Causal Principles for Improving Visual Dialog</li>
                <li>More Grounded Image Captioning by Distilling Image-Text Matching Model</li>
                <li>Counterfactual Samples Synthesizing for Robust Visual Question Answering</li>
                <li>Learning Filter Pruning Criteria for Deep Convolutional Neural Networks Acceleration</li>
            </div>

            <div data-date="Aug 2019">
                <h4><b>PREMIA Best Student Paper, Silver Award</b></h4>
                <li>Our paper "Learning to Compose Dynamic Tree Structures for Visual Contexts" received the PREMIA Best Student Paper, Silver Award (2nd Place)</li>
            </div>

            <div data-date="Jul 2019">
                <h4><b>Four Papers (Two Oral) Accepted by ICCV 2019</b></h4>
                <li>Counterfactual Critic Multi-Agent Training for Scene Graph Generation [<em
                    class="bluetext">oral</em>]</li>
                <li>Learning to Assemble Neural Module Tree Networks for Visual Grounding [<em
                    class="bluetext">oral</em>]</li>
                <li>Making History Matter: History-Advantage Sequence Training for Visual Dialog</li>
                <li>Learning to Collocate Neural Modules for Image Captioning</li>
            </div>

            <div data-date="Jul 2019">
                <h4><b>One Paper Accepted by TPAMI</b></h4>
            </div>

            <div data-date="Jul 2019">
                <h4><b>Three Papers Accepted by ACM MM 2019</b></h4>
            </div>

            <div data-date="Jun 2019">
                <h4><b>CVPR 2019 conference</b></h4>
                <li>Our Team <b>MReaL</b>-BDAI wins the first place in Visual Dialogue Challenge</li>
                <li>Our paper "Learning to Compose Dynamic Tree Structures for Visual Contexts" appears on the Best Paper Finallists.</li>
            </div>

            <div data-date="Apr 2019">
                <h4><b>Two Papers Accepted by TPAMI</b></h4>
            </div>

            <div data-date="Mar 2019">
                <h4><b> Four Papers (Three Oral) Accepted by CVPR 2019</b></h4>
            </div>

            <div data-date="Jan 2019">
                <h4><b>Two Papers Accepted by AAAI 2019</b></h4>
            </div>

        </div>
    </div>
    <!-- End #experience -->

    <div id="contact">
        <h2>Contact Us</h2>
        <div id="contact-form">
            <form method="POST" action="mailto:hanwangzhang@ntu.edu.sg">
                <input type="hidden" name="_subject" value="Contact request from personal website" />
                <input type="email" name="_replyto" placeholder="Your email" required>
                <textarea name="message" placeholder="Your message" required></textarea>
                <button type="submit">Send</button>
            </form>
        </div>
        <!-- End #contact-form -->
    </div>
    <!-- End #contact -->

    <footer>
        <div class="container">
            <div class="row">
                <div class="col-sm-5 copyright">
                    <p>
                        Copyright &copy; 2019 Machine Reasoning and Learning Lab
                    </p>
                </div>
                <div class="col-sm-2 top">
                    <span id="to-top">
                        <i class="fa fa-chevron-up" aria-hidden="true"></i>
                    </span>
                </div>
                <div class="col-sm-5 social">
                    <ul>
                        <li>
                            <a href="https://github.com/" target="_blank"><i class="fa fa-github"
                                    aria-hidden="true"></i></a>
                        </li>
                        <li>
                            <a href="https://stackoverflow.com/" target="_blank"><i class="fa fa-stack-overflow"
                                    aria-hidden="true"></i></a>
                        </li>
                        <li>
                            <a href="https://www.facebook.com/" target="_blank"><i class="fa fa-facebook"
                                    aria-hidden="true"></i></a>
                        </li>
                        <li>
                            <a href="https://twitter.com/" target="_blank"><i class="fa fa-twitter"
                                    aria-hidden="true"></i></a>
                        </li>
                        <li>
                            <a href="https://plus.google.com/" target="_blank"><i class="fa fa-google-plus"
                                    aria-hidden="true"></i></a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </footer>
    <!-- End footer -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="js/scripts.min.js"></script>
</body>

</html>
